{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import module\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.amazon.de/Relaxdays-Rankobelisk-beschichtetes-witterungsbest%C3%A4ndige-Rankhilfe/product-reviews/B004W1HEJ8/ref=cm_cr_arp_d_paging_btm_next_2?ie=UTF8&reviewerType=all_reviews&pageNumber={page}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmazonScraper:\n",
    "    HEADERS = ({'User-Agent':\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) \\\n",
    "        AppleWebKit/537.36 (KHTML, like Gecko) \\\n",
    "        Chrome/90.0.4430.212 Safari/537.36',\n",
    "        'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update(self.HEADERS)\n",
    "\n",
    "    def make_request(self, url) -> requests.models.Response:\n",
    "        \"\"\"Make a request to the given url and return the response\"\"\"\n",
    "        response = self.session.get(url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}\")\n",
    "        return response\n",
    "\n",
    "    def get_soup(self, response: requests.models.Response) -> BeautifulSoup:\n",
    "        \"\"\"Return a BeautifulSoup object from the given response\"\"\"\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return soup\n",
    "\n",
    "\n",
    "\n",
    "    def extract_reviews(self, soup: BeautifulSoup) -> list[str]:\n",
    "        reviews = []\n",
    "        outer_spans = soup.find_all('span', attrs={'data-hook': 'review-body'})\n",
    "        for outer_span in outer_spans:\n",
    "            try:\n",
    "                inner_span = outer_span.find('span')\n",
    "                review_text = inner_span.text.strip()\n",
    "                reviews.append(review_text)\n",
    "            except:\n",
    "                print(f\"skipping span {outer_span}\")\n",
    "        return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_response_content(response: requests.models.Response, path: Path):\n",
    "    with open(path, 'wb') as f:\n",
    "        f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Review:\n",
    "    title: str\n",
    "    text: str\n",
    "    rating: int\n",
    "\n",
    "    def to_file(self, path: Path):\n",
    "        with open(path, 'w') as f:\n",
    "            f.write(f'title: {self.title} \\n')\n",
    "            f.write(f'text: {self.text} \\n')\n",
    "            f.write(f'rating: {self.rating} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_customer_review_id(tag):\n",
    "    return tag.has_attr('id') and tag['id'].startswith('customer_review')\n",
    "\n",
    "\n",
    "def scrape_page(soup: BeautifulSoup, counter:int) -> int:\n",
    "    reviews_divs = soup.find_all(has_customer_review_id)\n",
    "    for review_div in reviews_divs:\n",
    "        print(f\"processing review {counter}\")\n",
    "        # Extract title\n",
    "        title = review_div.find('a', {'data-hook': 'review-title'}).text.strip()\n",
    "        \n",
    "        # Extract number of stars\n",
    "        stars = review_div.find('i', {'data-hook': 'review-star-rating'}).find('span').text.strip()\n",
    "        \n",
    "        # Extract review text - assuming it is contained in a p element with 'data-hook': 'review-body'\n",
    "        review_text = review_div.find('span', {'data-hook': 'review-body'}).text.strip()\n",
    "\n",
    "        review = Review(title=title, text=review_text, rating=stars)\n",
    "        review.to_file(Path(f'raw_reviews/review_{counter}.txt'))\n",
    "        counter += 1\n",
    "\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = AmazonScraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1\n",
      "processing review 0\n",
      "processing review 1\n",
      "processing review 2\n",
      "processing review 3\n",
      "processing review 4\n",
      "processing review 5\n",
      "processing review 6\n",
      "processing review 7\n",
      "processing review 8\n",
      "processing review 9\n",
      "Scraping page 2\n",
      "processing review 10\n",
      "processing review 11\n",
      "processing review 12\n",
      "processing review 13\n",
      "processing review 14\n",
      "processing review 15\n",
      "processing review 16\n",
      "processing review 17\n",
      "processing review 18\n",
      "processing review 19\n",
      "Scraping page 3\n",
      "processing review 20\n",
      "processing review 21\n",
      "processing review 22\n",
      "processing review 23\n",
      "processing review 24\n",
      "processing review 25\n",
      "processing review 26\n",
      "processing review 27\n",
      "processing review 28\n",
      "processing review 29\n",
      "Scraping page 4\n",
      "processing review 30\n",
      "processing review 31\n",
      "processing review 32\n",
      "processing review 33\n",
      "processing review 34\n",
      "processing review 35\n",
      "processing review 36\n",
      "processing review 37\n",
      "processing review 38\n",
      "processing review 39\n",
      "Scraping page 5\n",
      "processing review 40\n",
      "processing review 41\n",
      "processing review 42\n",
      "processing review 43\n",
      "processing review 44\n",
      "processing review 45\n",
      "processing review 46\n",
      "processing review 47\n",
      "processing review 48\n",
      "processing review 49\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for page in [1,2,3,4,5]:\n",
    "    print(f\"Scraping page {page}\")\n",
    "    response = scraper.make_request(url.format(page=page))\n",
    "    soup = scraper.get_soup(response)\n",
    "    counter = scrape_page(soup, counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
