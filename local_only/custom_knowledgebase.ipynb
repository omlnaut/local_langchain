{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.vectorstores.faiss import FAISS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-cpp-python in /home/omlnaut/mambaforge/envs/langchain/lib/python3.10/site-packages (0.1.42)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/omlnaut/mambaforge/envs/langchain/lib/python3.10/site-packages (from llama-cpp-python) (4.5.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ./models/ggml-model-q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =  68.20 KB\n",
      "llama_model_load_internal: mem required  = 5809.33 MB (+ 2052.00 MB per state)\n",
      "llama_init_from_file: kv self size  =  512.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
      "llama_model_load: loading model from './models/gpt4all-converted.bin' - please wait ...\n",
      "llama_model_load: n_vocab = 32001\n",
      "llama_model_load: n_ctx   = 512\n",
      "llama_model_load: n_embd  = 4096\n",
      "llama_model_load: n_mult  = 256\n",
      "llama_model_load: n_head  = 32\n",
      "llama_model_load: n_layer = 32\n",
      "llama_model_load: n_rot   = 128\n",
      "llama_model_load: f16     = 2\n",
      "llama_model_load: n_ff    = 11008\n",
      "llama_model_load: n_parts = 1\n",
      "llama_model_load: type    = 1\n",
      "llama_model_load: ggml map size = 4017.70 MB\n",
      "llama_model_load: ggml ctx size =  81.25 KB\n",
      "llama_model_load: mem required  = 5809.78 MB (+ 2052.00 MB per state)\n",
      "llama_model_load: loading tensors from './models/gpt4all-converted.bin'\n",
      "llama_model_load: model size =  4017.27 MB / num tensors = 291\n",
      "llama_init_from_file: kv self size  =  512.00 MB\n"
     ]
    }
   ],
   "source": [
    "gpt4all_path = './models/gpt4all-converted.bin' \n",
    "llama_path = './models/ggml-model-q4_0.bin' \n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "loader = TextLoader('./docs/shortened_sotu.txt')\n",
    "# embeddings = LlamaCppEmbeddings(model_path=llama_path)\n",
    "embeddings = LlamaCppEmbeddings(model_path=llama_path)\n",
    "llm = GPT4All(model=gpt4all_path, callback_manager=callback_manager, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_chunks(sources):\n",
    "    chunks = []\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=32)\n",
    "    for chunk in splitter.split_documents(sources):\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def create_index(chunks):\n",
    "    texts = [doc.page_content for doc in chunks]\n",
    "    metadatas = [doc.metadata for doc in chunks]\n",
    "\n",
    "    search_index = FAISS.from_texts(texts, embeddings, metadatas=metadatas)\n",
    "\n",
    "    return search_index\n",
    "\n",
    "\n",
    "def similarity_search(query, index):\n",
    "    matched_docs = index.similarity_search(query, k=4)\n",
    "    sources = []\n",
    "    for doc in matched_docs:\n",
    "        sources.append(\n",
    "            {\n",
    "                \"page_content\": doc.page_content,\n",
    "                \"metadata\": doc.metadata,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return matched_docs, sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-gpu in /home/omlnaut/mambaforge/envs/langchain/lib/python3.10/site-packages (1.7.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No index found. Creating index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  4599.72 ms /    61 tokens (   75.41 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  4618.03 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  4468.71 ms /    61 tokens (   73.26 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  4485.68 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  3169.31 ms /    43 tokens (   73.70 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  3181.46 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2652.84 ms /    37 tokens (   71.70 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2662.92 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2797.24 ms /    39 tokens (   71.72 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2807.72 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2042.65 ms /    28 tokens (   72.95 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2052.00 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2806.41 ms /    39 tokens (   71.96 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2817.37 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  3905.80 ms /    54 tokens (   72.33 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  3920.66 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  4031.76 ms /    56 tokens (   72.00 ms per token)\n",
      "llama_print_timings:        eval time =   123.65 ms /     1 runs   (  123.65 ms per run)\n",
      "llama_print_timings:       total time =  4173.30 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  4121.30 ms /    56 tokens (   73.59 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  4136.00 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2100.37 ms /    29 tokens (   72.43 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2109.11 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  3471.86 ms /    48 tokens (   72.33 ms per token)\n",
      "llama_print_timings:        eval time =   132.75 ms /     1 runs   (  132.75 ms per run)\n",
      "llama_print_timings:       total time =  3619.27 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2224.81 ms /    30 tokens (   74.16 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2233.94 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  4066.02 ms /    55 tokens (   73.93 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  4081.68 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  1167.52 ms /    16 tokens (   72.97 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  1172.28 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  3435.61 ms /    47 tokens (   73.10 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  3448.20 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  3461.37 ms /    48 tokens (   72.11 ms per token)\n",
      "llama_print_timings:        eval time =   120.88 ms /     1 runs   (  120.88 ms per run)\n",
      "llama_print_timings:       total time =  3596.99 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  3764.02 ms /    52 tokens (   72.38 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  3778.94 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2087.01 ms /    28 tokens (   74.54 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2095.98 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  4563.52 ms /    63 tokens (   72.44 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  4580.25 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2870.32 ms /    40 tokens (   71.76 ms per token)\n",
      "llama_print_timings:        eval time =   119.09 ms /     1 runs   (  119.09 ms per run)\n",
      "llama_print_timings:       total time =  3002.67 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  4281.46 ms /    58 tokens (   73.82 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  4298.93 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2702.60 ms /    37 tokens (   73.04 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2713.58 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  3837.41 ms /    53 tokens (   72.40 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  3853.02 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  3460.18 ms /    48 tokens (   72.09 ms per token)\n",
      "llama_print_timings:        eval time =   116.83 ms /     1 runs   (  116.83 ms per run)\n",
      "llama_print_timings:       total time =  3591.89 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2585.98 ms /    36 tokens (   71.83 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2596.91 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  3104.42 ms /    43 tokens (   72.20 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  3117.37 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2612.51 ms /    36 tokens (   72.57 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2623.24 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2618.22 ms /    36 tokens (   72.73 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2629.03 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2763.19 ms /    38 tokens (   72.72 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2774.73 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2670.65 ms /    37 tokens (   72.18 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2681.47 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2597.73 ms /    36 tokens (   72.16 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2609.16 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  3325.34 ms /    46 tokens (   72.29 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  3338.66 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2861.95 ms /    40 tokens (   71.55 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2872.48 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  3289.33 ms /    46 tokens (   71.51 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  3301.79 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  3313.26 ms /    46 tokens (   72.03 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  3326.76 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2178.85 ms /    30 tokens (   72.63 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2187.55 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2232.52 ms /    31 tokens (   72.02 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2241.82 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  3665.43 ms /    51 tokens (   71.87 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  3679.76 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2454.24 ms /    34 tokens (   72.18 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2464.83 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  4227.98 ms /    59 tokens (   71.66 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  4245.01 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  3878.23 ms /    54 tokens (   71.82 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  3893.99 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2516.36 ms /    35 tokens (   71.90 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2527.17 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2793.93 ms /    39 tokens (   71.64 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2804.63 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  3381.76 ms /    47 tokens (   71.95 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  3394.40 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  3110.51 ms /    43 tokens (   72.34 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  3123.28 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  4048.88 ms /    56 tokens (   72.30 ms per token)\n",
      "llama_print_timings:        eval time =   122.50 ms /     1 runs   (  122.50 ms per run)\n",
      "llama_print_timings:       total time =  4190.57 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  3310.80 ms /    46 tokens (   71.97 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  3323.65 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  4168.91 ms /    58 tokens (   71.88 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  4185.55 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2652.39 ms /    37 tokens (   71.69 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2662.99 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  3876.43 ms /    54 tokens (   71.79 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  3890.89 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  2460.97 ms /    34 tokens (   72.38 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  2471.50 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  3022.44 ms /    42 tokens (   71.96 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  3035.71 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  4021.27 ms /    56 tokens (   71.81 ms per token)\n",
      "llama_print_timings:        eval time =   121.80 ms /     1 runs   (  121.80 ms per run)\n",
      "llama_print_timings:       total time =  4159.92 ms\n",
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  4601.21 ms /    64 tokens (   71.89 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  4617.60 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-05-04 21:50:41,599] {loader.py:54} INFO - Loading faiss with AVX2 support.\n",
      "[2023-05-04 21:50:41,599] {loader.py:58} INFO - Could not load library with AVX2 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx2'\")\n",
      "[2023-05-04 21:50:41,600] {loader.py:64} INFO - Loading faiss.\n",
      "[2023-05-04 21:50:41,635] {loader.py:66} INFO - Successfully loaded faiss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  4542.26 ms /    63 tokens (   72.10 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  4558.39 ms\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "index_path = Path(\"models/state_of_the_union_index_gpu\")\n",
    "str_path = str(index_path)\n",
    "\n",
    "if not index_path.is_file():\n",
    "    print('No index found. Creating index...')\n",
    "    # Create Index\n",
    "    docs = loader.load()\n",
    "    chunks = split_chunks(docs)\n",
    "    index = create_index(chunks)\n",
    "    index.save_local(str_path)\n",
    "else:\n",
    "    print(f'Index found. Loading index from {index_path}...') \n",
    "    # path to string\n",
    "    index = FAISS.load_local(str_path, embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   703.56 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =   960.06 ms /    13 tokens (   73.85 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =   965.27 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4,\n",
       " [{'page_content': 'We are cutting off Russia’s largest banks from the international financial system.  \\n\\nPreventing Russia’s central bank from defending the Russian Ruble making Putin’s $630 Billion “war fund” worthless.',\n",
       "   'metadata': {'source': './docs/shortened_sotu.txt'}},\n",
       "  {'page_content': 'We are choking off Russia’s access to technology that will sap its economic strength and weaken its military for years to come.',\n",
       "   'metadata': {'source': './docs/shortened_sotu.txt'}},\n",
       "  {'page_content': 'Let each of us here tonight in this Chamber send an unmistakable signal to Ukraine and to the world. \\n\\nPlease rise if you are able and show that, Yes, we the United States of America stand with the Ukrainian people.',\n",
       "   'metadata': {'source': './docs/shortened_sotu.txt'}},\n",
       "  {'page_content': 'Putin’s latest attack on Ukraine was premeditated and unprovoked. \\n\\nHe rejected repeated efforts at diplomacy.',\n",
       "   'metadata': {'source': './docs/shortened_sotu.txt'}}])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Summarize the comments about NATO and its purpose.\"\n",
    "matched_docs, sources = similarity_search(question, index)\n",
    "len(matched_docs), sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_generate: seed = 1683229842\n",
      "\n",
      "system_info: n_threads = 4 / 16 | AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
      "sampling: temp = 0.800000, top_k = 40, top_p = 0.950000, repeat_last_n = 64, repeat_penalty = 1.300000\n",
      "generate: n_ctx = 512, n_batch = 1, n_predict = 256, n_keep = 0\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Please use the following context to answer questions.\n",
      "Context: We are cutting off Russia’s largest banks from the international financial system.  \n",
      "\n",
      "Preventing Russia’s central bank from defending the Russian Ruble making Putin’s $630 Billion “war fund” worthless.\n",
      "We are choking off Russia’s access to technology that will sap its economic strength and weaken its military for years to come.\n",
      "Let each of us here tonight in this Chamber send an unmistakable signal to Ukraine and to the world. \n",
      "\n",
      "Please rise if you are able and show that, Yes, we the United States of America stand with the Ukrainian people.\n",
      "Putin’s latest attack on Ukraine was premeditated and unprovoked. \n",
      "\n",
      "He rejected repeated efforts at diplomacy.\n",
      "---\n",
      "Question: Summarize the comments about NATO and its purpose.\n",
      "Answer: Let's think step by step. First, let me recap what I have seen so far in this debate. The United States of America stands with Ukraine; Putin’s latest attack on Ukrainians was premeditated and unprovoked. Secondly, NATO has a purpose to defend against external aggression as well as internal threats from within member states or allies who are under threat. Therefore, it is clear that NATO should be strengthened in order for its members’ safety measures. \n",
      "Please use the following context to answer questions.\n",
      "Context: We are cutting off Russia’s largest banks from the international financial system.  \n",
      "\n",
      "Preventing Russia’s central bank from defending the Russian Ruble making Putin’s $630 Billion “war fund” worthless.\n",
      "We are choking off Russia’s access to technology that will sap its economic strength and weaken its military for years to come.\n",
      "Let each of us here tonight in this Chamber send an unmistakable signal to Ukraine and to the world. \n",
      "\n",
      "Please rise if you are able and show that, Yes, we the United States of America stand with the Ukrainian people.\n",
      "Putin’s latest attack on Ukraine was premeditated and unprovoked. \n",
      "\n",
      "He rejected repeated efforts at diplomacy.\n",
      "---\n",
      "Question: Summarize the comments about NATO and its purpose.\n",
      "Answer: Let's think step by step. First, let me recap what I have seen so far in this debate. The United States of America stands with Ukraine; Putin’s latest attack on Ukrainians was premeditated and unprovoked. Secondly, NATO has a purpose to defend against external aggression as well as internal threats from within member states or allies who are under threat. Therefore, it is clear that NATO should be strengthened in order for its members’ safety measures.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [end of text]\n",
      "\n",
      "llama_print_timings:        load time = 234592.44 ms\n",
      "llama_print_timings:      sample time =    56.83 ms /    99 runs   (    0.57 ms per run)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token)\n",
      "llama_print_timings:        eval time = 158411.94 ms /   303 runs   (  522.81 ms per run)\n",
      "llama_print_timings:       total time = 343380.27 ms\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "Please use the following context to answer questions.\n",
    "Context: {context}\n",
    "---\n",
    "Question: {question}\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "context = \"\\n\".join([doc.page_content for doc in matched_docs])\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"]).partial(context=context)\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
